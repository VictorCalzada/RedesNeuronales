{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "tensorflow.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introducción a las convnets: Clasificando números\n",
    "\n",
    "\n",
    "\n",
    "Vamos a echarle un vistazo a un ejemplo sencillo de una convnet. La utilizaremos para clasificar el dataset MNIST, que es un dataset abierto que contiene números escritos a mano. \n",
    "\n",
    "![Números escritos a mano del dataset MNIST](http://corochann.com/wp-content/uploads/2017/02/mnist_plot.png)\n",
    "\n",
    "Vamos a crear una primera convnet basica. Es una pila de capas `Conv2D` y `MaxPooling2D`. \n",
    "Lo importante es notar que una convnet toma como input tensores de tamaño `(altura_imagen, anchura_imagen, canales_imagen)`. \n",
    "Para ello primero hay que averiguar el tamaño de las imágenes de nuestro dataset. \n",
    "\n",
    "La red debe tener las siguientes capas:\n",
    "\n",
    "- Una capa convolucional (Conv2D) con 32 filtros de 3x3 y activación relu. En esta primera capa deberás indicar el tamaño del input (input_shape).\n",
    "- Una segunda capa de Max Pooling (MaxPooling2D) de 2x2\n",
    "- Una tercera capa convolucional con 64 filtros de 3x3 y activación relu\n",
    "- Una cuarta capa de Max Pooling (MaxPooling2D) de 2x2\n",
    "- Una quinta capa convolucional de 64 filtros de 3x3 y activación relu\n",
    "\n",
    "Sabrás que lo has hecho bien cuando el output de model.summary() sea:\n",
    "\n",
    "![imagen_output.png](https://github.com/laramaktub/MachineLearningI/blob/master/imagen_output.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 00:24:15.630049: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-16 00:24:15.630210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-16 00:24:15.630709: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puedes ver arriba que la salida de cada capa `Conv2D` y `MaxPooling2D` es un tensor 3D de dimensiones `(altura, anchura, canales)`. La anchura y la altura tienden a diminuir según vamos yendo mas profundo en la red. El número de canales está controlado por el primer argumento que se le pasa a \n",
    "las capas `Conv2D`  (e.j. 32 o 64).\n",
    "\n",
    "El siguiente paso sería darle nuestro ultimo tensor (de dimensiones `(3, 3, 64)`) como entrada a una red densamente conectada. \n",
    "Estos clasificadores procesan vectores, que son 1D,  mientras que nuestra salida es un tensor 3D. \n",
    "Así que primero tendremos que aplanar nuestra salida 3D y convertirla en 1D y después añadir unas cuantas capas densas:\n",
    "\n",
    "- Primero aplana la salida (flatten())\n",
    "- Añade una primera capa de 64 neuronas y activación relu\n",
    "- Añade una última capa de 10 neuronas (tantas como números puedes clasificar) y activación softwmax\n",
    "\n",
    "Sabrás que lo has hecho bien cuando el summary tenga esta pinta:\n",
    "\n",
    "![imagen_output_flat.png](https://github.com/laramaktub/MachineLearningI/blob/master/imagen_output_flat.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos clasificar 10 categorías, lo que significa que nuestra capa final debe tener 10 nodos y una función de activación softmax. Vamos a ver que pinta tiene nuestra red:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, nuestra salida de dimension `(3, 3, 64)` han sido aplanadas hasta convertirse en vectores de dimensión `(576,)`, antes de entrar en las dos capas densas.\n",
    "\n",
    "Vamos ahora a entrenar nuestra red con las imágenes del dataset MNIST.\n",
    "\n",
    "Leemos a continuación el dataset y lo metemos dentro de vectores: train_images, train_labels, test_images, test_labels\n",
    "\n",
    "Antes de continuar, imprime:\n",
    "\n",
    "- ¿Cual es el tamaño del dataset de training?\n",
    "- ¿Qué pinta tiene el dataset de training?\n",
    "- ¿Qué pinta tienen las etiquetas de training?\n",
    "- Imprime la cuarta imagen del dataset de training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cual es el tamaño del dataset de training?\n",
      "El tamaño de train es: 60000\n",
      "\n",
      "¿Qué pinta tiene el dataset de training?\n",
      "La pinta del dataset train es: (60000, 28, 28, 1)\n",
      "\n",
      "¿Qué pinta tienen las etiquetas de training?\n",
      "Las etiquetas de train son: (60000, 10)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(f'¿Cual es el tamaño del dataset de training?\\nEl tamaño de train es: {len(train_images)}\\n')\n",
    "print(f'¿Qué pinta tiene el dataset de training?\\nLa pinta del dataset train es: {train_images.shape}\\n')\n",
    "print(f'¿Qué pinta tienen las etiquetas de training?\\nLas etiquetas de train son: {train_labels.shape}\\n')\n",
    "plt.imshow(train_images[19,:,:])\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vas a darle la forma adecuada a los datasets de training y test para poder meterlos a la red neuronal. Pasa las labels, que ahora mismo son números, a su forma categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compila el modelo indicando cuales son los datos de entrenamiento y sus etiquetas. Utilizando el optimizador \"rmsprop\" y como loss function usa la entropía cruzada categórica.\n",
    "Entrena después el modelo durante 5 épocas y un tamaño de batch de 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 00:24:19.326874: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-16 00:24:19.329331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3401065000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 41s 42ms/step - loss: 0.3747 - accuracy: 0.8796\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.0499 - accuracy: 0.9845\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 40s 43ms/step - loss: 0.0309 - accuracy: 0.9906\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 38s 41ms/step - loss: 0.0236 - accuracy: 0.9926\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 31s 33ms/step - loss: 0.0178 - accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa2e87c1760>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy' , optimizer=\"rmsprop\" , metrics=['accuracy'])\n",
    "model.fit(train_images,train_labels ,batch_size=64 , epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a evaluar el modelo con las imágenes de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 9ms/step - loss: 0.0306 - accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime la accuracy del test que acabas de realizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La accuracy es: 0.9918000102043152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'La accuracy es: {test_acc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea una imagen con un número escrito a mano y mira cual es la predicción. Prueba con unos cuantos números...¿Lo hace bien? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%run whiteboard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img_width=28\n",
    "img_height=28\n",
    "\n",
    "img = image.load_img('siete.png', target_size=(img_width, img_height),grayscale=True)\n",
    "x= np.abs(image.img_to_array(img)-255)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(x[0,:,:,:])\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('net_numbers.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga el modelo que acabas de guardar (load) y haz una predicción (predict_classes) con la imágen del número que acabas de escribir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('net_numbers.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
